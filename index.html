<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">@import url('https://themes.googleusercontent.com/fonts/css?kit=-GVb7k7Qu9uRSGSTITNRTJTjO3mf2QkTJ1928myN28I');.lst-kix_u0dww5ottopv-0>li{counter-increment:lst-ctn-kix_u0dww5ottopv-0}ol.lst-kix_u0dww5ottopv-1.start{counter-reset:lst-ctn-kix_u0dww5ottopv-1 0}ol.lst-kix_u0dww5ottopv-8.start{counter-reset:lst-ctn-kix_u0dww5ottopv-8 0}ol.lst-kix_u0dww5ottopv-0.start{counter-reset:lst-ctn-kix_u0dww5ottopv-0 0}.lst-kix_u0dww5ottopv-6>li{counter-increment:lst-ctn-kix_u0dww5ottopv-6}.lst-kix_ub6xupi29f7f-0>li:before{content:"  "}.lst-kix_u0dww5ottopv-7>li:before{content:"" counter(lst-ctn-kix_u0dww5ottopv-7,lower-latin) ". "}.lst-kix_u0dww5ottopv-8>li:before{content:"" counter(lst-ctn-kix_u0dww5ottopv-8,lower-roman) ". "}.lst-kix_ub6xupi29f7f-1>li:before{content:"\0025cb  "}.lst-kix_u0dww5ottopv-5>li:before{content:"" counter(lst-ctn-kix_u0dww5ottopv-5,lower-roman) ". "}ol.lst-kix_u0dww5ottopv-5.start{counter-reset:lst-ctn-kix_u0dww5ottopv-5 0}.lst-kix_ub6xupi29f7f-2>li:before{content:"\0025a0  "}.lst-kix_ub6xupi29f7f-3>li:before{content:"\0025cf  "}.lst-kix_ub6xupi29f7f-5>li:before{content:"\0025a0  "}.lst-kix_u0dww5ottopv-6>li:before{content:"" counter(lst-ctn-kix_u0dww5ottopv-6,decimal) ". "}ol.lst-kix_u0dww5ottopv-2.start{counter-reset:lst-ctn-kix_u0dww5ottopv-2 0}.lst-kix_ub6xupi29f7f-4>li:before{content:"\0025cb  "}.lst-kix_u0dww5ottopv-7>li{counter-increment:lst-ctn-kix_u0dww5ottopv-7}.lst-kix_ub6xupi29f7f-7>li:before{content:"\0025cb  "}.lst-kix_u0dww5ottopv-1>li{counter-increment:lst-ctn-kix_u0dww5ottopv-1}.lst-kix_ub6xupi29f7f-6>li:before{content:"\0025cf  "}.lst-kix_u0dww5ottopv-4>li{counter-increment:lst-ctn-kix_u0dww5ottopv-4}.lst-kix_ub6xupi29f7f-8>li:before{content:"\0025a0  "}ol.lst-kix_u0dww5ottopv-4{list-style-type:none}ol.lst-kix_u0dww5ottopv-3{list-style-type:none}ol.lst-kix_u0dww5ottopv-2{list-style-type:none}ol.lst-kix_u0dww5ottopv-1{list-style-type:none}ul.lst-kix_ub6xupi29f7f-1{list-style-type:none}ol.lst-kix_u0dww5ottopv-0{list-style-type:none}ul.lst-kix_ub6xupi29f7f-0{list-style-type:none}ol.lst-kix_u0dww5ottopv-6.start{counter-reset:lst-ctn-kix_u0dww5ottopv-6 0}ul.lst-kix_ub6xupi29f7f-5{list-style-type:none}ul.lst-kix_ub6xupi29f7f-4{list-style-type:none}.lst-kix_u0dww5ottopv-0>li:before{content:"" counter(lst-ctn-kix_u0dww5ottopv-0,lower-latin) ") "}ul.lst-kix_ub6xupi29f7f-3{list-style-type:none}ul.lst-kix_ub6xupi29f7f-2{list-style-type:none}ol.lst-kix_u0dww5ottopv-3.start{counter-reset:lst-ctn-kix_u0dww5ottopv-3 0}ol.lst-kix_u0dww5ottopv-8{list-style-type:none}ul.lst-kix_ub6xupi29f7f-8{list-style-type:none}.lst-kix_u0dww5ottopv-1>li:before{content:"" counter(lst-ctn-kix_u0dww5ottopv-1,lower-latin) ". "}ol.lst-kix_u0dww5ottopv-7{list-style-type:none}ul.lst-kix_ub6xupi29f7f-7{list-style-type:none}ol.lst-kix_u0dww5ottopv-6{list-style-type:none}ul.lst-kix_ub6xupi29f7f-6{list-style-type:none}ol.lst-kix_u0dww5ottopv-5{list-style-type:none}.lst-kix_u0dww5ottopv-3>li:before{content:"" counter(lst-ctn-kix_u0dww5ottopv-3,decimal) ". "}.lst-kix_u0dww5ottopv-4>li:before{content:"" counter(lst-ctn-kix_u0dww5ottopv-4,lower-latin) ". "}.lst-kix_u0dww5ottopv-3>li{counter-increment:lst-ctn-kix_u0dww5ottopv-3}.lst-kix_u0dww5ottopv-2>li:before{content:"" counter(lst-ctn-kix_u0dww5ottopv-2,lower-roman) ". "}ol.lst-kix_u0dww5ottopv-7.start{counter-reset:lst-ctn-kix_u0dww5ottopv-7 0}.lst-kix_u0dww5ottopv-5>li{counter-increment:lst-ctn-kix_u0dww5ottopv-5}.lst-kix_u0dww5ottopv-2>li{counter-increment:lst-ctn-kix_u0dww5ottopv-2}.lst-kix_u0dww5ottopv-8>li{counter-increment:lst-ctn-kix_u0dww5ottopv-8}ol.lst-kix_u0dww5ottopv-4.start{counter-reset:lst-ctn-kix_u0dww5ottopv-4 0}ol{margin:0;padding:0}table td,table th{padding:0}.c20{color:#ff0000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Courier New";font-style:normal}.c27{color:#a5a5a5;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Roboto Mono";font-style:normal}.c9{margin-left:18pt;padding-top:3pt;padding-bottom:6pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c10{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:14pt;font-family:"Roboto Mono";font-style:normal}.c28{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:60pt;font-family:"Courier New";font-style:normal}.c29{color:#a5a5a5;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Courier New";font-style:normal}.c2{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Courier New";font-style:normal}.c1{padding-top:2pt;padding-bottom:18pt;line-height:1.0;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c13{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:8pt;font-family:"Roboto Mono";font-style:normal}.c15{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Courier New";font-style:normal}.c22{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Calibri";font-style:normal}.c0{color:#ff0000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:25pt;font-family:"Courier New";font-style:normal}.c23{padding-top:10pt;padding-bottom:4pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c25{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c19{padding-top:10pt;padding-bottom:6pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c3{padding-top:20pt;padding-bottom:6pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c7{color:#000000;font-weight:400;vertical-align:baseline;font-size:10pt;font-family:"Courier New";font-style:normal}.c26{padding-top:4pt;padding-bottom:6pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c17{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Courier New"}.c4{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline}.c5{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;text-decoration:underline}.c18{background-color:#ffffff;max-width:451.4pt;padding:72pt 72pt 72pt 72pt}.c24{font-size:60pt;font-family:"Roboto Mono";font-weight:400}.c11{color:#ff0000;font-size:25pt}.c6{color:inherit;text-decoration:inherit}.c16{height:10pt}.c8{font-style:italic}.c14{page-break-after:avoid}.c21{height:14pt}.c12{font-size:25pt}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Courier New";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:10pt;font-family:"Courier New"}p{margin:0;color:#000000;font-size:10pt;font-family:"Courier New"}h1{padding-top:20pt;color:#ff0000;font-size:18pt;padding-bottom:6pt;font-family:"Courier New";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:2pt;color:#000000;font-size:14pt;padding-bottom:18pt;font-family:"Roboto Mono";line-height:1.0;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Courier New";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Courier New";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Courier New";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Courier New";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c18"><p class="c3"><span class="c24">VRD</span></p><p class="c3"><span class="c0">///</span></p><p class="c3"><span class="c0">Content</span></p><p class="c3 c16"><span class="c15"></span></p><p class="c26"><span class="c5"><a class="c6" href="#h.d6bgxm1unczv">About</a></span></p><p class="c9"><span class="c5"><a class="c6" href="#h.yohue1bt2fo0">Welcome to the Video Reuse Detector (VRD).</a></span></p><p class="c9"><span class="c5"><a class="c6" href="#h.40n8rulw893g">Scientific rationale</a></span></p><p class="c19"><span class="c5"><a class="c6" href="#h.molw3id4w1mk">Readme</a></span></p><p class="c9"><span class="c5"><a class="c6" href="#h.687s011rw7kg">Availability</a></span></p><p class="c9"><span class="c5"><a class="c6" href="#h.jfia4lfd29wh">Technical overview</a></span></p><p class="c9"><span class="c5"><a class="c6" href="#h.faq2mw4hgckn">A brief introduction to convolutional neural nets (CNN&rsquo;s)</a></span></p><p class="c9"><span class="c5"><a class="c6" href="#h.2utidxcxmw9">The VRDs use of CNN&rsquo;s</a></span></p><p class="c9"><span class="c5"><a class="c6" href="#h.kg5zmrryfs96">Similarity search using Faiss</a></span></p><p class="c9"><span class="c5"><a class="c6" href="#h.eyruhq6jpuiy">Filtering options</a></span></p><p class="c9"><span class="c5"><a class="c6" href="#h.6p4c521lkrfo">Matching results</a></span></p><p class="c9"><span class="c5"><a class="c6" href="#h.m2qscopcov06">Toolkit limitations</a></span></p><p class="c19"><span class="c5"><a class="c6" href="#h.a8ib17svlt9d">Install</a></span></p><p class="c9"><span class="c5"><a class="c6" href="#h.sz8w4yugape2">Download Docker</a></span></p><p class="c9"><span class="c5"><a class="c6" href="#h.iwxnx84csmmk">Install the VRD Docker container</a></span></p><p class="c9"><span class="c5"><a class="c6" href="#h.5o52p643e4wo">Adjust local configurations</a></span></p><p class="c9"><span class="c5"><a class="c6" href="#h.myvyn0sjfdbi">Access the VRD via Jupyter Labs</a></span></p><p class="c23"><span class="c5"><a class="c6" href="#h.8ziwm978n3gk">Contact</a></span></p><p class="c3 c16"><span class="c20"></span></p><p class="c3"><span class="c11">///</span></p><h1 class="c3 c14" id="h.d6bgxm1unczv"><span class="c12">About</span></h1><p class="c3 c16"><span class="c2"></span></p><h2 class="c1" id="h.yohue1bt2fo0"><span class="c10">Welcome to the Video Reuse Detector (VRD)</span></h2><p class="c3"><span>The VRD is </span><span>a methodological toolkit for identifying visual similarities in audiovisual archives with the help of machine learning methods. It </span><span class="c2">has been assembled because of the lack of open-source solutions for audiovisual copy detection and is meant to help archivists and humanistic scholars study video reuse. </span></p><p class="c3"><span>By automating the process of detecting how, when, and where video content reemerges within a given archive, the VRD allows you to match and</span><span>&nbsp;compare a) one or several selected reference videos against a larger database, or b) all videos within a database against each other.</span></p><p class="c3"><span>The toolkit has been developed within the research project </span><span class="c4"><a class="c6" href="https://www.google.com/url?q=https://www.cadeah.eu/&amp;sa=D&amp;source=editors&amp;ust=1633081445484000&amp;usg=AOvVaw1Xn-A88-o74Z8lcw7_ytwp">European History Reloaded: Curation and Appropriation of Digital Audiovisual Heritage</a></span><span>, funded by the JPI Cultural Heritage project, EU Horizon 2020 research and innovation programme. Its main developer is </span><span class="c4"><a class="c6" href="https://www.google.com/url?q=https://www.umu.se/en/staff/tomas-skotare/&amp;sa=D&amp;source=editors&amp;ust=1633081445485000&amp;usg=AOvVaw3YU-ZbCn4bKifj6fOiDmxn">Tomas Skotare</a></span><span>, with assistance from </span><span class="c4"><a class="c6" href="https://www.google.com/url?q=https://www.umu.se/en/staff/maria-c-eriksson/&amp;sa=D&amp;source=editors&amp;ust=1633081445485000&amp;usg=AOvVaw0uDcz-Hzec9fTcgzsf6xyC">Maria Eriksson</a></span><span>&nbsp;and </span><span class="c4"><a class="c6" href="https://www.google.com/url?q=https://www.umu.se/en/staff/pelle-snickars/&amp;sa=D&amp;source=editors&amp;ust=1633081445486000&amp;usg=AOvVaw0qT5c4ipiodCkASvMQMFIR">Pelle Snickars</a></span><span>. </span></p><p class="c3 c16"><span class="c2"></span></p><h2 class="c1" id="h.40n8rulw893g"><span class="c10">Scientific rationale </span></h2><p class="c3"><span>Our work with the VRD is inspired by the </span><span class="c4"><a class="c6" href="https://www.google.com/url?q=https://academic.oup.com/dsh/article/35/1/194/5296356&amp;sa=D&amp;source=editors&amp;ust=1633081445487000&amp;usg=AOvVaw3cHbbMlJ45yZ1qv8yapw6b">&ldquo;visual turn&rdquo;</a></span><span>&nbsp;in historic and digitally-oriented humanities research, where a growing number of scholars have recently shifted their attention towards working with visual and audiovisual sources (see for example </span><span class="c4"><a class="c6" href="https://www.google.com/url?q=https://www.distantviewing.org/&amp;sa=D&amp;source=editors&amp;ust=1633081445487000&amp;usg=AOvVaw1xounL2fqGIoNJzgym5VAF">Arnold and Tilton</a></span><span>, </span><span class="c4"><a class="c6" href="https://www.google.com/url?q=https://academic.oup.com/dsh/article/35/1/194/5296356&amp;sa=D&amp;source=editors&amp;ust=1633081445488000&amp;usg=AOvVaw2yV8zEyXg3zHH4NsBBpKNT">Wevers and Smits</a></span><span>, </span><span class="c4"><a class="c6" href="https://www.google.com/url?q=https://mitpress.mit.edu/books/cultural-analytics&amp;sa=D&amp;source=editors&amp;ust=1633081445488000&amp;usg=AOvVaw3OCvqiFzjL9jYk7QAcCCQB">Manovich</a></span><span>, </span><span class="c4"><a class="c6" href="https://www.google.com/url?q=https://hdl.handle.net/2027/fulcrum.70795899c&amp;sa=D&amp;source=editors&amp;ust=1633081445489000&amp;usg=AOvVaw1kXdxIjnTCEo9KI5fs6zUH">Kee and Campeau</a></span><span class="c2">). </span></p><p class="c3"><span>The VRD builds on and extends these research efforts, but it also differs from existing tools in one important way: while many audiovisual toolkits in the digital humanities involving machine learning are focused on automating the production of semantic metadata &ndash; and use machines to annotate </span><span class="c8">who</span><span>&nbsp;or </span><span class="c8">what</span><span class="c2">&nbsp;appears in images &ndash; our tool is solely focused on searching for visual similarities within a given dataset. </span></p><p class="c3"><span>In short, we are interested in mapping the &ldquo;social life&rdquo; or &ldquo;cultural biographies&rdquo; of individual video clips (to borrow from </span><span class="c4"><a class="c6" href="https://www.google.com/url?q=https://www.cambridge.org/core/books/social-life-of-things/4F4D3929A501EC19CF413D36BDF8AB3A&amp;sa=D&amp;source=editors&amp;ust=1633081445490000&amp;usg=AOvVaw1NnQTmaEgNr7cQ9W9Jvosd">Arjun Appadurai</a></span><span>&nbsp;and </span><span class="c4"><a class="c6" href="https://www.google.com/url?q=https://www.cambridge.org/core/books/social-life-of-things/4F4D3929A501EC19CF413D36BDF8AB3A&amp;sa=D&amp;source=editors&amp;ust=1633081445490000&amp;usg=AOvVaw1NnQTmaEgNr7cQ9W9Jvosd">Igor Kopytoff</a></span><span class="c2">). This, for example, involves exploring how the meaning of historic footage changes when it circulates and is recycled/cross-referenced in video productions through time. </span></p><p class="c3"><span class="c2">How, for instance, has a selected video &ndash; say, footage from the moon landing in 1969 &ndash; been reused in documentary films and news broadcasts throughout history? When and where has this footage played a role in extraterrestrial imaginaries, or potentially helped shape narratives concerning fundamentally different topics?</span></p><p class="c3"><span class="c2">Answering such questions by analyzing large reference archives manually would be very time-consuming, creating incentives for the development of tools that can automatically reduce the workload. The VRD is such a tool and helps speed up the process of identifying reuse in large audiovisual databases. </span></p><p class="c3"><span class="c2">Aside from being used in academic settings, we hope the VRD can help archives and cultural heritage institutions find excessive copies in digitized collections and otherwise assist in finding patterns of similarity across audiovisual databases.</span></p><p class="c3 c16"><span class="c29"></span></p><h1 class="c3 c14" id="h.molw3id4w1mk"><span class="c0">///</span></h1><h1 class="c3 c14" id="h.lf4nuilferse"><span class="c0">Readme</span></h1><p class="c3 c16"><span class="c2"></span></p><h2 class="c1" id="h.687s011rw7kg"><span class="c10">Availability </span></h2><p class="c3"><span>The VRD is embedded in </span><span class="c4"><a class="c6" href="https://www.google.com/url?q=https://jupyter.org/&amp;sa=D&amp;source=editors&amp;ust=1633081445492000&amp;usg=AOvVaw2vA6LWKqWxzaKmg7FjTkWg">Jupyter Notebooks</a></span><span class="c2">&nbsp;&ndash; an interactive computing environment and programming interface that allows users to run customized code through their web browsers. </span></p><p class="c3"><span class="c2">We chose to work with Jupyter Notebooks because it is especially good for guiding someone through a computational workflow and explaining the basic logic and ideas behind software. Jupyter notebooks can also easily be extended, modified, and transformed in line with the user&rsquo;s preferences, making it a suitable starting point for developing a research project.</span></p><p class="c3"><span>Jupyter Notebooks is free to use and download and there are several tutorials and guides that explain how the tool works. </span><span class="c4"><a class="c6" href="https://www.google.com/url?q=https://www.dataquest.io/blog/jupyter-notebook-tutorial/&amp;sa=D&amp;source=editors&amp;ust=1633081445493000&amp;usg=AOvVaw0SpLHWNI4ZuLpf-kf3K-fa">This is a good place to start</a></span><span class="c2">. </span></p><p class="c3 c16"><span class="c2"></span></p><h2 class="c1" id="h.jfia4lfd29wh"><span class="c10">Technical overview</span></h2><p class="c3"><span>The VRD uses machine learning techniques (more specifically, </span><span class="c4"><a class="c6" href="https://www.google.com/url?q=https://en.wikipedia.org/wiki/Convolutional_neural_network&amp;sa=D&amp;source=editors&amp;ust=1633081445495000&amp;usg=AOvVaw1yR-9YcF0CsnV5YChmGPTG">convolutional neural networks</a></span><span>, or CNN&rsquo;s), combined with tools for performing similarity searches (more specifically, the </span><span class="c4"><a class="c6" href="https://www.google.com/url?q=https://ai.facebook.com/tools/faiss/&amp;sa=D&amp;source=editors&amp;ust=1633081445495000&amp;usg=AOvVaw3f6KGntgQJkfIx1Wx6UqH2">Faiss library</a></span><span class="c2">) to detect copies in audiovisual archives. </span></p><p class="c3"><span class="c2">The VRD also assembles a series of tools for trimming and preparing datasets (including software for removing black borders and monochrome video frames), and filtering/visualizing matching results (such as introducing similarity thresholds, filtering based on sequential matches of frames, and visually viewing the final matching results). </span></p><p class="c3"><span>We decided to work with CNN&rsquo;s and the Faiss library after doing multiple tests with more traditional tools for visual content recognition, including video fingerprinting with help of </span><span class="c4"><a class="c6" href="https://www.google.com/url?q=https://en.wikipedia.org/wiki/Oriented_FAST_and_rotated_BRIEF&amp;sa=D&amp;source=editors&amp;ust=1633081445496000&amp;usg=AOvVaw3ua233iwLxnJJLAvZLvQEH">ORB</a></span><span class="c2">&nbsp;(Oriented FAST and rotated BRIEF) &ndash; a method for extracting and comparing visual features in images. The combination of CNN&rsquo;s and Faiss quickly outperformed the ORB technology&rsquo;s ways of identifying visual similarities in video content, however, both in terms of accuracy and processing speed.</span></p><p class="c3 c16"><span class="c2"></span></p><h2 class="c1" id="h.faq2mw4hgckn"><span class="c10">A brief introduction to convolutional neural nets (CNN&rsquo;s)</span></h2><p class="c3"><span>C</span><span>onvolutional neural nets, or CNN&rsquo;s, constitute a machine learning technique that is frequently used to extract and analyze visual content. You can read more about how CNNs work </span><span class="c4"><a class="c6" href="https://www.google.com/url?q=https://en.wikipedia.org/wiki/Deep_learning%23Deep_neural_networks&amp;sa=D&amp;source=editors&amp;ust=1633081445497000&amp;usg=AOvVaw1VJzCIjWQFuwqp6KxLzm5A">here</a></span><span>&nbsp;or </span><span class="c4"><a class="c6" href="https://www.google.com/url?q=https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53&amp;sa=D&amp;source=editors&amp;ust=1633081445497000&amp;usg=AOvVaw3CGp3zQoOE3vHd9Rtr4DzB">here</a></span><span class="c2">, but put in simple terms, the technology is inspired by a model of the connectivity patterns of neurons in the animal visual cortex. &nbsp;</span></p><p class="c3"><span class="c2">CNN&rsquo;s are for example used in medical image analysis, image recommendation systems, and &ndash; importantly for our purposes here &ndash; to automatically search for and identify similarities in images or videos. </span></p><p class="c3"><span>While the detailed technical workings of individual CNNs may differ, the technology is broadly designed according to multiple layers of analysis and abstraction. Each layer in a CNN processes an input and produces an output, which is passed on to the next layer. </span></p><p class="c3"><span class="c2">For instance, one layer in a CNN may observe how pixels are spatially arranged and search for areas with a high contrast between nearby pixels (a good marker for what is visually unique in a particular image), while another layer might focus on reducing what information is stored about pixel contrasts (instructing the model to &ldquo;forget&rdquo; all areas in a picture with a lower pixel contrast than a given value, for example). In this way, the CNN produces a successively smaller and hopefully more precise &ldquo;map&rdquo; of the analyzed image. </span></p><p class="c3"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 245.33px; height: 243.58px;"><img alt="" src="images/image2.png" style="width: 245.33px; height: 243.58px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span>&nbsp; </span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 269.50px; height: 247.48px;"><img alt="" src="images/image1.png" style="width: 269.50px; height: 247.48px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c3"><span class="c13">Example of what a processed frame may look like at one layer of the analysis when a CNN is applied to analyze video content. Here, we can assume that the CNN was searching for pixel contrasts by transforming the color settings in the analyzed frame (Image courtesy: European &nbsp;History Reloaded Team). </span></p><p class="c3"><span class="c2">Somewhere before the top layers of the analysis is reached, a CNN produces a compressed and final interpretation of the key visual characteristics of an image. It is then common for the remaining layers of a CNN to be trained/designed to annotate or classify the content in images by estimating what objects, animals, and human faces appear in the footage, for example. </span></p><h2 class="c1 c21" id="h.3aaczcbtxqx1"><span class="c27"></span></h2><h2 class="c1" id="h.2utidxcxmw9"><span class="c10">The VRDs use of CNN&rsquo;s</span></h2><p class="c3"><span class="c2">In order to use a CNN, the VRD begins its analysis by dividing each video in a designated dataset into still frames. Commonly, a digital video consists of 24-30 frames per second, and the VRD is pre-set to extract one such frame for every second of video.</span></p><p class="c3"><span class="c2">The VRD then applies a CNN to process the individual frames but stops when the final (and compressed) interpretation of an image has been produced. These key visual features are exported as-is and because of their highly abstracted nature, they are somewhat resilient to modifications in the source data. This means that an image can be recognized even if someone has adjusted its color, resolution, or composition, for example.</span></p><p class="c3"><span class="c2">That the VRD stops before the top layers in the CNN have been reached, means that the tool does not analyze and annotate what things, people, or places appear in images. Instead, it uses the compressed visual features to find patterns of similarity across images. This shortens the image processing time and is especially customized for studying video reuse.</span></p><p class="c3"><span>Currently, the four different convolutional neural networks that come with </span><span class="c4"><a class="c6" href="https://www.google.com/url?q=https://keras.io/&amp;sa=D&amp;source=editors&amp;ust=1633081445500000&amp;usg=AOvVaw2BHb3OYiw7ITJGGxKKvcdc">Keras</a></span><span>&nbsp;&ndash; a so-called &ldquo;wrapper&rdquo; for CNN&rsquo;s &ndash; can be used with the VRD: </span><span class="c4"><a class="c6" href="https://www.google.com/url?q=https://www.mathworks.com/help/deeplearning/ref/resnet50.html&amp;sa=D&amp;source=editors&amp;ust=1633081445501000&amp;usg=AOvVaw2JKsG5B5BsDjUD0lEnvqnZ">ResNet50</a></span><span>, </span><span class="c4"><a class="c6" href="https://www.google.com/url?q=https://cloud.google.com/tpu/docs/inception-v3-advanced&amp;sa=D&amp;source=editors&amp;ust=1633081445501000&amp;usg=AOvVaw378pAsHQpNxPGhO-7Wz4LB">Inception_3</a></span><span>, </span><span class="c4"><a class="c6" href="https://www.google.com/url?q=https://arxiv.org/abs/1505.06798&amp;sa=D&amp;source=editors&amp;ust=1633081445502000&amp;usg=AOvVaw3lL5jy-_oS-WxDjsDqSlnU">VGG16</a></span><span>, and </span><span class="c4"><a class="c6" href="https://www.google.com/url?q=https://arxiv.org/abs/1704.04861&amp;sa=D&amp;source=editors&amp;ust=1633081445502000&amp;usg=AOvVaw2sW6KBfSVh8w5R9y8gKJIx">MobileNets</a></span><span>. Each of these CNN&rsquo;s apply their own method for extracting visual features from images but they all exhibit a layered convolutional structure. What also combines them, is that they have been trained and validated using the </span><span class="c4"><a class="c6" href="https://www.google.com/url?q=http://www.image-net.org/&amp;sa=D&amp;source=editors&amp;ust=1633081445503000&amp;usg=AOvVaw0fWMlJ9L7LnHtFO4VY7RaW">ImageNet dataset</a></span><span>&nbsp;(or more precisely, the so-called </span><span class="c4"><a class="c6" href="https://www.google.com/url?q=https://www.pyimagesearch.com/2017/03/20/imagenet-vggnet-resnet-inception-xception-keras/&amp;sa=D&amp;source=editors&amp;ust=1633081445503000&amp;usg=AOvVaw3lGXpqC6uODA_egi_oM4SE">ImageNet Large Scale Visual Recognition Challenge</a></span><span class="c2">) which is arguably the world&rsquo;s most frequently used dataset for developing and testing machine vision tools. </span></p><p class="c3"><span class="c4"><a class="c6" href="https://www.google.com/url?q=https://www.mathworks.com/help/deeplearning/ref/resnet50.html&amp;sa=D&amp;source=editors&amp;ust=1633081445504000&amp;usg=AOvVaw1sqxavlCeLQuEVajForTw0">ResNet50</a></span><span>&nbsp;and </span><span class="c4"><a class="c6" href="https://www.google.com/url?q=https://arxiv.org/abs/1505.06798&amp;sa=D&amp;source=editors&amp;ust=1633081445504000&amp;usg=AOvVaw1xoECBbnWXCCGTqmLYt26z">VGG-16</a></span><span>&nbsp;have both been developed at Microsoft Research, while </span><span class="c4"><a class="c6" href="https://www.google.com/url?q=https://cloud.google.com/tpu/docs/inception-v3-advanced&amp;sa=D&amp;source=editors&amp;ust=1633081445505000&amp;usg=AOvVaw2inggCsAUYYMDDlko2gtnk">Inception_V3</a></span><span>&nbsp;and </span><span class="c4"><a class="c6" href="https://www.google.com/url?q=https://arxiv.org/abs/1704.04861&amp;sa=D&amp;source=editors&amp;ust=1633081445505000&amp;usg=AOvVaw2WwTCIR_3XsusDHzoWw2Vi">MobileNets</a></span><span>&nbsp;have been developed at Google. It is difficult to say which one of these neural nets is &ldquo;better&rdquo; in the study of video reuse and they all come with individual </span><span class="c4"><a class="c6" href="https://www.google.com/url?q=https://www.pyimagesearch.com/2017/03/20/imagenet-vggnet-resnet-inception-xception-keras/&amp;sa=D&amp;source=editors&amp;ust=1633081445506000&amp;usg=AOvVaw3aBEA83k1YxXtofNm_cHR4">benefits and drawbacks</a></span><span class="c2">. </span></p><p class="c3"><span>Previous tests have shown that the performance and accuracy of the tools are </span><span class="c4"><a class="c6" href="https://www.google.com/url?q=https://keras.io/api/applications/&amp;sa=D&amp;source=editors&amp;ust=1633081445506000&amp;usg=AOvVaw2AT5rz7VR1sNlyrnj3wktf">comparatively similar</a></span><span class="c2">, apart from the fact that the VGG-16 neural net is fairly large and uses quite a lot of disk space and bandwidth. It should be noted, however, that most performance tests of the above mentioned CNN&rsquo;s have focused on their capacity to correctly annotate/classify the content in images (faces, animals, objects, etc.), and not their ability to assist in the detection of visual similarities, which is how the CNN&rsquo;s are used in the VRD.</span></p><p class="c3"><span class="c2">In our experiments with the VRD, we have mostly relied on ResNet50, although we encourage users to experiment and examine which CNN works best for their individual video dataset. While the current version of the VRD uses the CNNs that are included in Keras, there is also no reason why additional networks could not be applied in the future, potentially further improving performance. </span></p><p class="c3 c16"><span class="c2"></span></p><h2 class="c1" id="h.kg5zmrryfs96"><span class="c10">Similarity search using Faiss</span></h2><p class="c3"><span>When the image processing with a CNN is finished, the VRD uses the so-called </span><span class="c4"><a class="c6" href="https://www.google.com/url?q=https://ai.facebook.com/tools/faiss/&amp;sa=D&amp;source=editors&amp;ust=1633081445508000&amp;usg=AOvVaw2NrOmQ77y9BN6EPKucv04-">Faiss</a></span><span class="c2">&nbsp;(Facebook AI Similarity Search) library to calculate image similarity. Faiss was first released in 2019 and as the name suggests, it is developed by Facebook. Currently, it is considered to be one of the most efficient open-source tools for conducting large-scale similarity searches. For instance, Faiss can be run on Graphical Processing Units (GPU&rsquo;s) which provides significant advantages in terms of speed. While Faiss can be used for any sort of similarity search, we use it here to identify similarities between images.</span></p><p class="c3"><span class="c2">Faiss uses the visual features that were extracted in the previous step to index and calculate the visually most similar &ldquo;neighbors&rdquo; for each video frame. In more detail, all reference images are first added to a Faiss index. New images are then compared with all images in the index, producing an arbitrarily long (as defined by the user) list of similarity neighbours. Requesting more neighbours requires more processing time, meaning that the number of neighbours that the VRD is instructed to find is a tradeoff between time and accuracy. </span></p><p class="c3"><span class="c2">The found neighbours are each sorted by distance metric from the compared image, with lower numbers representing a more similar image. The distance metric 0.0 represents the absolute closest similarity match that the VRD can identify. What distance metric represents a &ldquo;correct&rdquo; match is hard to determine in a general sense, however, as it changes with the choice of a neural network, the quality of the source material, and the number of videos/images/frames in the index, for example.</span></p><p class="c3 c16"><span class="c2"></span></p><h2 class="c1" id="h.eyruhq6jpuiy"><span class="c10">Filtering options</span></h2><p class="c3"><span class="c2">To tailor and narrowing down the search results, the beta version of the VRD includes a number of filtering options. For example, it is possible to filter out the display of any identified similarity neighbors that come from the same video. It is also possible to filter out monochrome frames from the search results, assuming that monochrome frames (such as totally black or white frames) are of less interest to the user. </span></p><p class="c3"><span class="c2">Users are also given the option to select a threshold for which distance metrics should be shown in the final matching results. For instance, the VRD may be instructed to only show frame pairs with an assigned distance metric of less than 30 000. If this threshold is accurate, it should greatly reduce the number of shown non-matching frames. </span></p><p class="c16 c25"><span class="c22"></span></p><h2 class="c1" id="h.6p4c521lkrfo"><span class="c10">Matching results</span></h2><p class="c3"><span class="c2">When the VRD&rsquo;s image processing and filtering is finished, the final matching results are shown to the user in textual and visual form. </span></p><p class="c3"><span class="c2">For instance, the user can study the matching results in tables that show which individual frames received the most identified similarity neighbours, or what frame pairs (i.e. comparisons between two different frames stemming from two different videos) received the lowest distance metrics.</span></p><p class="c3"><span class="c2">[Insert image example]</span></p><p class="c3"><span class="c2">Users can also explore visual samples of identified similarity matches between videos, by studying miniature images of the analyzed frames.</span></p><p class="c3"><span class="c2">[Insert image example]</span></p><p class="c3"><span class="c2">In addition, the VRD is equipped with possibilities to preview so-called sequential frame matches. We define a sequential match as an instance where two or more sequential frames (i.e. frames that were extracted one after the other from the original files) from two videos have been given a distance metric below a specified value. </span></p><p class="c3"><span class="c2">If frame 1-6 in Video X and frame 11-16 in Video Y are each given a distance metric below the threshold 20 000, for example, this may be defined as a sequential match. </span></p><p class="c3"><span class="c2">Sequential filtering can be used to identify instances when longer chunks of moving images have been reused, &nbsp;and can for example be filtered according to the length of sequences (such as 3 seconds, 4 seconds or 5 seconds long sequential matches), or identified sequential matches for individual videos. </span></p><p class="c3"><span class="c2">[Insert image example]</span></p><p class="c3"><span class="c2">On the whole, the above-mentioned lists, tables, and visual frame comparisons are meant to function as a guide that can point users towards videos that might be interesting to study in more detail &ndash; primarily by actually opening the original video files and viewing the moving images. </span></p><p class="c3"><span>In other words, we strongly advise against exporting and using these statistics as absolute proof of video reuse and instead encourage users to approach them as an </span><span class="c8">assistance tool</span><span class="c2">&nbsp;in navigating large video datasets.</span></p><p class="c3 c16"><span class="c2"></span></p><h2 class="c1" id="h.m2qscopcov06"><span class="c10">Toolkit limitations</span></h2><p class="c3"><span class="c2">In its current version, the VRD only analyzes visual content and disregards sound. This means that the tool cannot detect if the audio track of a video file has been reused and reappears somewhere else. The rationale for this is that reused video is often dubbed/overlayed with different audio, meaning that the audio might not match even in cases where the video is reused. Since we were originally interested in performing visual similarity searches, adding audio features to the VRD has not been a priority at this stage. There is no reason why sound recognition technologies could not be incorporated in the tool in the future, however. </span></p><p class="c3"><span>By testing and analyzing the VRD&rsquo;s matching results against the </span><span class="c4"><a class="c6" href="https://www.google.com/url?q=http://www.yugangjiang.info/research/VCDB/index.html&amp;sa=D&amp;source=editors&amp;ust=1633081445513000&amp;usg=AOvVaw05auCA3gopgK9uasj9x6s6">VCDB dataset</a></span><span class="c2">&nbsp;(a large collection of remixed and manually annotated videos originating from video platforms such as YouTube), we have identified a few areas where the tool in its current state produces subpar results. One instance when the VRD&rsquo;s performance generally suffers is when the source video is of very low quality, such as when it is heavily distorted/modified, has a low-resolution, or has heavy textual or symbolic overlays (such as subtitles, logotypes, and news show banners). </span></p><p class="c3"><span class="c2">Furthermore, the VRD has difficulties recognizing content if it is faced with the challenge of identifying content shown on a hand-filmed TV or computer screen (a problem that is generally known as picture-in-picture). Another source of concern has been video content with large amounts of text, since text can easily be matched against text in other videos, even if the precise use of words are not identical. </span></p><p class="c3"><span class="c2">Regardless of these limitations, however, there is still a good chance that the VRD will correctly identify even highly modified or overlaid content &ndash; a chance that can in many cases be further improved by pre-processing.</span></p><p class="c3"><span>Finally, it is important to point out that the VRD produces an </span><span class="c8">estimation </span><span class="c2">of patterns of similarity across video files. This means that its outputs/results should not be interpreted as a definitive measurement of similarity or a final and absolute proof of video reuse. </span></p><p class="c3 c16"><span class="c15"></span></p><p class="c3"><span class="c0">///</span></p><h1 class="c3 c14" id="h.a8ib17svlt9d"><span class="c0">Install</span></h1><p class="c3 c16"><span class="c2"></span></p><h2 class="c1" id="h.sz8w4yugape2"><span class="c10">Download Docker</span></h2><p class="c3"><span class="c2">The VRD is designed to be run and installed through Docker &ndash; a third-party application that enables the use of containerized applications. To begin with, you must therefore download and install the Docker software.</span></p><p class="c3"><span class="c4"><a class="c6" href="https://www.google.com/url?q=https://docs.docker.com/get-docker/&amp;sa=D&amp;source=editors&amp;ust=1633081445515000&amp;usg=AOvVaw2JI7b5kJk4dMvYNmsGEtC7">Click here</a></span><span class="c2">&nbsp;to find the Docker installation packages and instructions for your operating system. </span></p><p class="c3 c16"><span class="c2"></span></p><h2 class="c1" id="h.iwxnx84csmmk"><span class="c10">Install the VRD Docker container </span></h2><p class="c3"><span>When Docker is installed, you can download and install the specialized VRD docker container by clicking on </span><span>this link.</span><span class="c2">&nbsp;</span></p><p class="c3"><span class="c2">This will create a docker image that contains all the necessary software packages required to run the VRD. </span></p><p class="c3 c16"><span class="c2"></span></p><h2 class="c1" id="h.5o52p643e4wo"><span class="c10">Adjust local configurations</span></h2><p class="c3"><span class="c2">Before the VRD is ready to be used, you need to edit the docker-compose.yml file to match your local file system. </span></p><p class="c3"><span class="c2">Open this file in your terminal and look for lines 14-15, which should say:</span></p><p class="c3"><span class="c2">-&#39;/{localpath}/VRD/Docker/ai-notebook/mount/Notebooks/:/home/jovyan/notebooks&#39;</span></p><p class="c3"><span class="c2">- &#39;/{localpath}/VRD/Videos:/home/jovyan/Videos/&#39;</span></p><p class="c3"><span class="c2">Then change both {localpath} lines to correctly match the directory you downloaded to. </span></p><p class="c3"><span class="c17 c8">[insert example of what this might look like, which elements need to be changed]</span></p><p class="c3"><span class="c2">When this is done, open a terminal (or equivalent) and navigate to the VRD/Docker directory on your system, and type docker-compose up. </span></p><p class="c3"><span class="c2">This should start the creation of the docker container, although it may take some time and require a couple of gigabytes of data to be downloaded. </span></p><p class="c3 c16"><span class="c2"></span></p><h2 class="c1" id="h.myvyn0sjfdbi"><span class="c10">Access the VRD via Jupyter Labs</span></h2><p class="c3"><span>We suggest you view and use the VRD notebook via the web interface </span><span class="c4"><a class="c6" href="https://www.google.com/url?q=https://blog.jupyter.org/jupyterlab-is-ready-for-users-5a6f039b8906&amp;sa=D&amp;source=editors&amp;ust=1633081445519000&amp;usg=AOvVaw3fQOqwjDHydlJdbn1ocm_e">Jupyter Labs</a></span><span class="c2">. </span></p><p class="c3"><span class="c2">When the previous steps in the installation guide have been finished, the docker container shown in your terminal should present you with a link to a homepage, similar to this:</span></p><p class="c3"><span class="c2">http://127.0.0.1:9003/lab/tree/notebooks/AI%20Notebook%20-%20SVT%201909.ipynb</span></p><p class="c3"><span>Copy and insert this link </span><span class="c2">in your preferred web browser. You should now be able to access the VRD via Jupyter Labs and start working on your project.</span></p><p class="c3 c16"><span class="c15"></span></p><h1 class="c3 c14" id="h.8ziwm978n3gk"><span class="c0">///</span></h1><h1 class="c3 c14" id="h.9be9akvpktj1"><span class="c12">Contact</span></h1><p class="c3"><span>Follow our project on </span><span class="c4"><a class="c6" href="https://www.google.com/url?q=https://github.com/inidun&amp;sa=D&amp;source=editors&amp;ust=1633081445521000&amp;usg=AOvVaw1u1I7G6hPE2jEa9cctIQcI">GitHub</a></span><span>&nbsp;and the </span><span class="c4"><a class="c6" href="https://www.google.com/url?q=https://www.cadeah.eu/&amp;sa=D&amp;source=editors&amp;ust=1633081445521000&amp;usg=AOvVaw19hrLPghJO_Mj-cK3oCaRb">European History Reloaded website</a></span><span class="c2">. </span></p><p class="c3"><span>Don&rsquo;t hesitate to reach out to </span><span class="c4"><a class="c6" href="mailto:tomas.skotare@umu.se">tomas.skotare@umu.se</a></span><span>&nbsp;or </span><span class="c4"><a class="c6" href="mailto:maria.c.eriksson@umu.se">maria.c.eriksson@umu.se</a></span><span class="c2">&nbsp;if you have any questions or comments.</span></p><p class="c3 c16"><span class="c2"></span></p></body></html>