<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">@import url('https://themes.googleusercontent.com/fonts/css?kit=-GVb7k7Qu9uRSGSTITNRTJTjO3mf2QkTJ1928myN28I');ol{margin:0;padding:0}table td,table th{padding:0}.c1{-webkit-text-decoration-skip:none;color:#000000;font-weight:400;text-decoration:underline;vertical-align:baseline;text-decoration-skip-ink:none;font-size:10pt;font-family:"Courier New";font-style:normal}.c2{margin-left:18pt;padding-top:3pt;padding-bottom:0pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c26{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Courier New";font-style:normal}.c25{color:#a5a5a5;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Roboto Mono";font-style:normal}.c17{color:#ff0000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:25pt;font-family:"Courier New";font-style:normal}.c3{padding-top:2pt;padding-bottom:18pt;line-height:1.0;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c30{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:60pt;font-family:"Courier New";font-style:normal}.c13{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:8pt;font-family:"Roboto Mono";font-style:normal}.c9{color:#ff0000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Courier New";font-style:normal}.c4{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Courier New";font-style:normal}.c23{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Calibri";font-style:normal}.c16{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c8{color:#ff0000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:25pt;font-family:"Roboto Mono";font-style:normal}.c6{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:14pt;font-family:"Roboto Mono";font-style:normal}.c27{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Roboto Mono";font-style:normal}.c12{color:#000000;font-weight:400;vertical-align:baseline;font-size:12pt;font-family:"Roboto Mono";font-style:normal}.c19{padding-top:20pt;padding-bottom:6pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c24{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c21{padding-top:10pt;padding-bottom:4pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c0{padding-top:20pt;padding-bottom:6pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c29{padding-top:4pt;padding-bottom:0pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c15{padding-top:10pt;padding-bottom:0pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c32{color:#ff0000;text-decoration:none;vertical-align:baseline;font-style:normal}.c11{color:#000000;text-decoration:none;vertical-align:baseline;font-style:italic}.c34{color:#a5a5a5;text-decoration:none;vertical-align:baseline;font-style:normal}.c7{-webkit-text-decoration-skip:none;text-decoration:underline;text-decoration-skip-ink:none}.c33{background-color:#ffffff;max-width:451.4pt;padding:72pt 72pt 72pt 72pt}.c31{font-size:60pt;font-family:"Roboto Mono";font-weight:400}.c28{font-size:8pt;font-family:"Roboto Mono";font-weight:400}.c5{font-size:10pt;font-family:"Courier New";font-weight:400}.c10{color:inherit;text-decoration:inherit}.c20{page-break-after:avoid}.c14{height:11pt}.c22{font-style:italic}.c18{height:16pt}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c33"><p class="c19"><span class="c31">VRD</span></p><p class="c19"><span class="c27">Beta version 0.1</span></p><p class="c19 c14"><span class="c27"></span></p><p class="c0"><span class="c8">///</span></p><p class="c0"><span class="c8">Contents</span></p><p class="c0 c14"><span class="c5 c32"></span></p><p class="c29"><span class="c12 c7"><a class="c10" href="#h.d6bgxm1unczv">About</a></span></p><p class="c2"><span class="c1"><a class="c10" href="#h.gvz48xktspsk">Welcome to the Video Reuse Detector (VRD)</a></span></p><p class="c2"><span class="c1"><a class="c10" href="#h.40n8rulw893g">Scientific rationale</a></span></p><p class="c15"><span class="c12 c7"><a class="c10" href="#h.lf4nuilferse">Readme</a></span></p><p class="c2"><span class="c1"><a class="c10" href="#h.687s011rw7kg">Availability</a></span></p><p class="c2"><span class="c1"><a class="c10" href="#h.jfia4lfd29wh">Technical overview</a></span></p><p class="c2"><span class="c1"><a class="c10" href="#h.faq2mw4hgckn">A brief introduction to convolutional neural nets (CNN&rsquo;s)</a></span></p><p class="c2"><span class="c1"><a class="c10" href="#h.2utidxcxmw9">The VRDs use of CNN&rsquo;s</a></span></p><p class="c2"><span class="c1"><a class="c10" href="#h.kg5zmrryfs96">Similarity search using Faiss</a></span></p><p class="c2"><span class="c1"><a class="c10" href="#h.eyruhq6jpuiy">Filtering options</a></span></p><p class="c2"><span class="c1"><a class="c10" href="#h.68l2nu8a3qm3">Matching results</a></span></p><p class="c2"><span class="c1"><a class="c10" href="#h.m2qscopcov06">Toolkit limitations</a></span></p><p class="c15"><span class="c12 c7"><a class="c10" href="#h.a8ib17svlt9d">Install</a></span></p><p class="c2"><span class="c7 c5"><a class="c10" href="#h.rwb267ta71be">** Work in progress **</a></span></p><p class="c21"><span class="c7 c12"><a class="c10" href="#h.9be9akvpktj1">Contact</a></span></p><p class="c0 c14"><span class="c9"></span></p><p class="c0"><span class="c8">///</span></p><h1 class="c0 c20" id="h.d6bgxm1unczv"><span class="c8">About</span></h1><h2 class="c3 c18" id="h.yohue1bt2fo0"><span class="c6"></span></h2><h2 class="c3" id="h.gvz48xktspsk"><span class="c6">Welcome to the Video Reuse Detector (VRD)</span></h2><p class="c0"><span class="c4">The VRD is a methodological toolkit for identifying visual similarities in audiovisual archives with the help of machine learning methods. It has been assembled because of the lack of open source solutions for audiovisual copy detection and is meant to help archivists and humanistic scholars study video reuse. </span></p><p class="c0"><span class="c4">The VRD allows you to match and compare a) one or several selected reference videos against a larger database, or b) all videos within a database against each other, and simplifies the process of detecting how, when, and where video content reemerges within a given archive. </span></p><p class="c0"><span class="c5">The toolkit has been developed within the research project </span><span class="c7 c5"><a class="c10" href="https://www.google.com/url?q=https://www.cadeah.eu/&amp;sa=D&amp;source=editors&amp;ust=1633504748330000&amp;usg=AOvVaw1A9H7-7W4nrvF123D-feIh">European History Reloaded: Curation and Appropriation of Digital Audiovisual Heritage</a></span><span class="c5">, funded by the JPI Cultural Heritage project, EU Horizon 2020 research and innovation program. Its main developer is </span><span class="c7 c5"><a class="c10" href="https://www.google.com/url?q=https://www.umu.se/en/staff/tomas-skotare/&amp;sa=D&amp;source=editors&amp;ust=1633504748330000&amp;usg=AOvVaw1NhXrJkQJhlx7OsplJRIIA">Tomas Skotare</a></span><span class="c5">, with assistance from </span><span class="c7 c5"><a class="c10" href="https://www.google.com/url?q=https://www.umu.se/en/staff/maria-c-eriksson/&amp;sa=D&amp;source=editors&amp;ust=1633504748331000&amp;usg=AOvVaw0ITGMxT0W8K2qrjVHEGESZ">Maria Eriksson</a></span><span class="c5">&nbsp;and </span><span class="c7 c5"><a class="c10" href="https://www.google.com/url?q=https://www.umu.se/en/staff/pelle-snickars/&amp;sa=D&amp;source=editors&amp;ust=1633504748331000&amp;usg=AOvVaw3QtvXz3nZE057aZet7WJt7">Pelle Snickars</a></span><span class="c5">. </span></p><p class="c0 c14"><span class="c4"></span></p><h2 class="c3" id="h.40n8rulw893g"><span class="c6">Scientific rationale </span></h2><p class="c0"><span class="c5">Our work with the VRD is inspired by the &ldquo;visual turn&rdquo; in historic and digitally-oriented humanities research, where a growing number of scholars have recently shifted their attention towards visual and audiovisual sources (see for example </span><span class="c7 c5"><a class="c10" href="https://www.google.com/url?q=https://academic.oup.com/dsh/article/35/1/194/5296356&amp;sa=D&amp;source=editors&amp;ust=1633504748332000&amp;usg=AOvVaw1Mk1aC2WEEW-Akm4HQw-WT">Wevers and Smits</a></span><span class="c5">, </span><span class="c7 c5"><a class="c10" href="https://www.google.com/url?q=https://www.distantviewing.org/&amp;sa=D&amp;source=editors&amp;ust=1633504748333000&amp;usg=AOvVaw2r_Y7yD7ircWXa_vwoyoJu">Arnold and Tilton</a></span><span class="c5">, </span><span class="c7 c5"><a class="c10" href="https://www.google.com/url?q=https://mitpress.mit.edu/books/cultural-analytics&amp;sa=D&amp;source=editors&amp;ust=1633504748333000&amp;usg=AOvVaw0CbTXKuKuOdSVc1vE8LguQ">Manovich</a></span><span class="c5">, </span><span class="c7 c5"><a class="c10" href="https://www.google.com/url?q=https://hdl.handle.net/2027/fulcrum.70795899c&amp;sa=D&amp;source=editors&amp;ust=1633504748333000&amp;usg=AOvVaw3crVDS3-aSzJCxxa4awcPh">Kee and Campeau</a></span><span class="c4">). </span></p><p class="c0"><span class="c5">The VRD builds on and extends these research efforts, but it also differs from existing tools in one important way: while many audiovisual toolkits in the digital humanities involving machine learning are focused on automating the production of semantic metadata &ndash; and use machines to annotate </span><span class="c5 c22">who</span><span class="c5">&nbsp;or </span><span class="c5 c22">what</span><span class="c4">&nbsp;appears in images &ndash; the VRD is solely focused on searching for visual similarities within a given dataset. </span></p><p class="c0"><span class="c5">In short, the VRD is designed to help map the &ldquo;social life&rdquo; or &ldquo;cultural biographies&rdquo; of individual video clips (to borrow from </span><span class="c7 c5"><a class="c10" href="https://www.google.com/url?q=https://www.cambridge.org/core/books/social-life-of-things/4F4D3929A501EC19CF413D36BDF8AB3A&amp;sa=D&amp;source=editors&amp;ust=1633504748334000&amp;usg=AOvVaw3ALjPZx2u7B5186r_xeEkE">Arjun Appadurai</a></span><span class="c5">&nbsp;and </span><span class="c7 c5"><a class="c10" href="https://www.google.com/url?q=https://www.cambridge.org/core/books/social-life-of-things/4F4D3929A501EC19CF413D36BDF8AB3A&amp;sa=D&amp;source=editors&amp;ust=1633504748334000&amp;usg=AOvVaw3ALjPZx2u7B5186r_xeEkE">Igor Kopytoff</a></span><span class="c4">). This, for example, involves exploring how the meaning of historic footage changes when it circulates and is recycled/cross-referenced in video productions through time. </span></p><p class="c0"><span class="c4">How, for instance, has a selected video &ndash; say, footage from the moon landing in 1969 &ndash; been reused in documentary films and news broadcasts throughout history? When and where has this footage played a role in extraterrestrial imaginaries, or potentially helped shape narratives concerning fundamentally different topics?</span></p><p class="c0"><span class="c4">Answering such questions by analyzing large reference archives manually would be very time-consuming, creating incentives for the development of tools that can automatically reduce the workload. The VRD is such a tool and helps speed up the process of identifying reuse in large audiovisual databases. </span></p><p class="c0"><span class="c4">Aside from being used in academic settings, we hope the VRD can help archives and cultural heritage institutions find excessive copies in digitized collections and otherwise assist in finding patterns of similarity across audiovisual databases.</span></p><p class="c0 c14"><span class="c5 c34"></span></p><p class="c0"><span class="c17">///</span></p><h1 class="c0 c20" id="h.lf4nuilferse"><span class="c8">Readme</span></h1><p class="c0 c14"><span class="c4"></span></p><h2 class="c3" id="h.687s011rw7kg"><span class="c6">Availability </span></h2><p class="c0"><span class="c5">The VRD is embedded in </span><span class="c7 c5"><a class="c10" href="https://www.google.com/url?q=https://jupyter.org/&amp;sa=D&amp;source=editors&amp;ust=1633504748335000&amp;usg=AOvVaw2IiwYsPL0Z396ybwaxO-9D">Jupyter Notebooks</a></span><span class="c4">&nbsp;&ndash; an interactive computing environment and programming interface that allows users to run customized code through their web browsers. </span></p><p class="c0"><span class="c4">We chose to work with Jupyter Notebooks because it is especially good for guiding someone through a computational workflow and explaining the basic logic and ideas behind software. A Jupyter notebook can also easily be extended, modified, and transformed in line with user preferences, making it a suitable starting point for developing a research project.</span></p><p class="c0"><span class="c5">Jupyter Notebooks is free to use and download and there are several tutorials and guides that explain how the tool works. </span><span class="c7 c5"><a class="c10" href="https://www.google.com/url?q=https://www.dataquest.io/blog/jupyter-notebook-tutorial/&amp;sa=D&amp;source=editors&amp;ust=1633504748336000&amp;usg=AOvVaw0HrXWswAsRkrm4ixCXOjI5">This is a good place to start</a></span><span class="c4">. </span></p><p class="c0 c14"><span class="c4"></span></p><h2 class="c3" id="h.jfia4lfd29wh"><span class="c6">Technical overview</span></h2><p class="c0"><span class="c5">The VRD uses machine learning techniques (more specifically, </span><span class="c7 c5"><a class="c10" href="https://www.google.com/url?q=https://en.wikipedia.org/wiki/Convolutional_neural_network&amp;sa=D&amp;source=editors&amp;ust=1633504748337000&amp;usg=AOvVaw2RSV2MC0efb9PeHMcOvvEY">convolutional neural networks</a></span><span class="c5">), combined with tools for performing similarity searches (more specifically, the </span><span class="c7 c5"><a class="c10" href="https://www.google.com/url?q=https://ai.facebook.com/tools/faiss/&amp;sa=D&amp;source=editors&amp;ust=1633504748337000&amp;usg=AOvVaw1QdXibzZBVAj5iGx6Ed8TL">Faiss library</a></span><span class="c4">) to detect copies in audiovisual archives. </span></p><p class="c0"><span class="c4">The VRD also assembles a series of tools for trimming and preparing datasets (including software for removing black borders and monochrome video frames), and filtering/visualizing matching results (such as introducing similarity thresholds, filtering based on sequential matches of frames, and visually viewing the final matching results). </span></p><p class="c0"><span class="c5">We decided to work with CNN&rsquo;s and the Faiss library after doing multiple tests with more traditional tools for visual content recognition, including video fingerprinting with help of </span><span class="c7 c5"><a class="c10" href="https://www.google.com/url?q=https://en.wikipedia.org/wiki/Oriented_FAST_and_rotated_BRIEF&amp;sa=D&amp;source=editors&amp;ust=1633504748338000&amp;usg=AOvVaw2MBhUz8LfxNyyKxDOpqAOz">ORB</a></span><span class="c4">&nbsp;(Oriented FAST and rotated BRIEF) &ndash; a method for extracting and comparing visual features in images. The combination of CNN&rsquo;s and Faiss quickly outperformed the ORB technology&rsquo;s ways of identifying visual similarities in video content, however, both in terms of accuracy and processing speed.</span></p><p class="c0 c14"><span class="c4"></span></p><h2 class="c3" id="h.faq2mw4hgckn"><span class="c6">A brief introduction to convolutional neural nets (CNN&rsquo;s)</span></h2><p class="c0"><span class="c5">Convolutional neural nets, or CNN&rsquo;s, constitute a machine learning technique that is frequently used to analyze visual content. You can read more about how CNNs work </span><span class="c7 c5"><a class="c10" href="https://www.google.com/url?q=https://en.wikipedia.org/wiki/Deep_learning%23Deep_neural_networks&amp;sa=D&amp;source=editors&amp;ust=1633504748339000&amp;usg=AOvVaw1nosNznR7411cqGEqcJEx2">here</a></span><span class="c5">&nbsp;or </span><span class="c5 c7"><a class="c10" href="https://www.google.com/url?q=https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53&amp;sa=D&amp;source=editors&amp;ust=1633504748340000&amp;usg=AOvVaw0zhifcR7MykMGFFuOmuUW_">here</a></span><span class="c4">, but put in simple terms, the technology is inspired by a model of the connectivity patterns of neurons in the animal visual cortex. &nbsp;</span></p><p class="c0"><span class="c4">CNN&rsquo;s are for example used in medical image analysis, image recommendation systems, and &ndash; importantly for our purposes here &ndash; to automatically search for and identify similarities in images or videos. </span></p><p class="c0"><span class="c5">While the detailed technical workings of individual CNNs may differ, the technology is broadly designed according to multiple layers of analysis and abstraction. Each layer in a CNN processes an input and produces an output, which is passed on to the next layer. </span></p><p class="c0"><span class="c4">For instance, one layer in a CNN may observe how pixels are spatially arranged and search for areas with a high contrast between nearby pixels (a good marker for what is visually unique in a particular image), while another layer might focus on reducing what information is stored about pixel contrasts (instructing the model to &ldquo;forget&rdquo; all areas in a picture with a lower pixel contrast than a given value, for example). In this way, the CNN produces a successively smaller and hopefully more precise &ldquo;map&rdquo; of the analyzed image. </span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 245.33px; height: 243.58px;"><img alt="" src="images/image1.png" style="width: 245.33px; height: 243.58px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span class="c5">&nbsp; </span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 269.50px; height: 247.48px;"><img alt="" src="images/image2.png" style="width: 269.50px; height: 247.48px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c13">Example of what a processed frame may look like at one layer of the analysis when a CNN is applied to analyze video content. Here, we can assume that the CNN was searching for pixel contrasts by transforming the color settings in the analyzed frame (image courtesy: European &nbsp;History Reloaded Team). </span></p><p class="c0"><span class="c4">Somewhere before the top layers of the analysis is reached, a CNN produces a compressed and final interpretation of the key visual characteristics of an image. It is then common for the remaining layers of a CNN to be trained/designed to annotate or classify the content in images by estimating what objects, animals, and human faces appear in the footage, for example. </span></p><h2 class="c3 c18" id="h.3aaczcbtxqx1"><span class="c25"></span></h2><h2 class="c3" id="h.2utidxcxmw9"><span class="c6">The VRDs use of CNN&rsquo;s</span></h2><p class="c0"><span class="c4">In order to use a CNN, the VRD begins its analysis by dividing each video in a designated dataset into still frames. Commonly, a digital video consists of 24-30 frames per second, and the VRD is pre-set to extract one such frame for every second of video.</span></p><p class="c0"><span class="c4">The VRD then applies a CNN to process the individual frames but stops when the final (and compressed) interpretation of an image has been produced. These key visual features are exported as-is and because of their highly abstracted nature, they are somewhat resilient to modifications in the source data. This means that an image can be recognized even if someone has adjusted its color, resolution, or composition, for example.</span></p><p class="c0"><span class="c4">That the VRD stops before the top layers in the CNN have been reached, means that the tool does not analyze and annotate what things, people, or places appear in images. Instead, it uses the compressed visual features to find patterns of similarity across images. This shortens the image processing time and is specially customized for studying video reuse.</span></p><p class="c0"><span class="c5">Currently, the four different convolutional neural networks that come with </span><span class="c7 c5"><a class="c10" href="https://www.google.com/url?q=https://keras.io/&amp;sa=D&amp;source=editors&amp;ust=1633504748342000&amp;usg=AOvVaw0P7Jkfshmo4vIpeTAO4hOS">Keras</a></span><span class="c5">&nbsp;&ndash; a so-called &ldquo;wrapper&rdquo; for CNN&rsquo;s &ndash; can be used with the VRD: </span><span class="c7 c5"><a class="c10" href="https://www.google.com/url?q=https://www.mathworks.com/help/deeplearning/ref/resnet50.html&amp;sa=D&amp;source=editors&amp;ust=1633504748343000&amp;usg=AOvVaw1w922WxdzX2LZkNZhSjxEc">ResNet50</a></span><span class="c5">, </span><span class="c7 c5"><a class="c10" href="https://www.google.com/url?q=https://cloud.google.com/tpu/docs/inception-v3-advanced&amp;sa=D&amp;source=editors&amp;ust=1633504748343000&amp;usg=AOvVaw3PvbzgByFlMazo6F6BNjmr">Inception_3</a></span><span class="c5">, </span><span class="c7 c5"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/1505.06798&amp;sa=D&amp;source=editors&amp;ust=1633504748344000&amp;usg=AOvVaw2ybz92PHs2bp_6H5V5fHKv">VGG16</a></span><span class="c5">, and </span><span class="c7 c5"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/1704.04861&amp;sa=D&amp;source=editors&amp;ust=1633504748344000&amp;usg=AOvVaw3hCkHKhDRWAWtxb1KPGm0Z">MobileNets</a></span><span class="c5">. Each of these CNN&rsquo;s apply their own method for extracting visual features from images but they all exhibit a layered convolutional structure. What also combines them, is that they have been trained and validated using the </span><span class="c7 c5"><a class="c10" href="https://www.google.com/url?q=http://www.image-net.org/&amp;sa=D&amp;source=editors&amp;ust=1633504748344000&amp;usg=AOvVaw3_fgJehgfQdBZKIvSVtmsM">ImageNet dataset</a></span><span class="c5">&nbsp;(or more precisely, the so-called </span><span class="c7 c5"><a class="c10" href="https://www.google.com/url?q=https://www.pyimagesearch.com/2017/03/20/imagenet-vggnet-resnet-inception-xception-keras/&amp;sa=D&amp;source=editors&amp;ust=1633504748345000&amp;usg=AOvVaw3RVOjVZlrpEu-vZmvceV7c">ImageNet Large Scale Visual Recognition Challenge</a></span><span class="c4">) which is arguably the world&rsquo;s most frequently used dataset for developing and testing machine vision tools. </span></p><p class="c0"><span class="c7 c5"><a class="c10" href="https://www.google.com/url?q=https://www.mathworks.com/help/deeplearning/ref/resnet50.html&amp;sa=D&amp;source=editors&amp;ust=1633504748345000&amp;usg=AOvVaw2A8G8Z9gLAt2TZjKXWFySN">ResNet50</a></span><span class="c5">&nbsp;and </span><span class="c7 c5"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/1505.06798&amp;sa=D&amp;source=editors&amp;ust=1633504748345000&amp;usg=AOvVaw0CXv6almt3hK8OUKXn-zQJ">VGG-16</a></span><span class="c5">&nbsp;have both been developed at Microsoft Research, while </span><span class="c7 c5"><a class="c10" href="https://www.google.com/url?q=https://cloud.google.com/tpu/docs/inception-v3-advanced&amp;sa=D&amp;source=editors&amp;ust=1633504748346000&amp;usg=AOvVaw015mkCRAeNAUw7sDMTWv_c">Inception_V3</a></span><span class="c5">&nbsp;and </span><span class="c7 c5"><a class="c10" href="https://www.google.com/url?q=https://arxiv.org/abs/1704.04861&amp;sa=D&amp;source=editors&amp;ust=1633504748346000&amp;usg=AOvVaw299-3kMSSEfqHAdDNG7_sz">MobileNets</a></span><span class="c5">&nbsp;have been developed at Google. It is difficult to say which one of these neural nets is &ldquo;better&rdquo; in the study of video reuse and they all come with individual </span><span class="c7 c5"><a class="c10" href="https://www.google.com/url?q=https://www.pyimagesearch.com/2017/03/20/imagenet-vggnet-resnet-inception-xception-keras/&amp;sa=D&amp;source=editors&amp;ust=1633504748346000&amp;usg=AOvVaw2TOdNJOv0nECSY8NFbZZNT">benefits and drawbacks</a></span><span class="c4">. </span></p><p class="c0"><span class="c5">Previous tests have shown that the performance and accuracy of the tools are </span><span class="c7 c5"><a class="c10" href="https://www.google.com/url?q=https://keras.io/api/applications/&amp;sa=D&amp;source=editors&amp;ust=1633504748347000&amp;usg=AOvVaw2AEpg3rR4bhiCL0Hr70NWf">comparatively similar</a></span><span class="c4">, apart from the fact that the VGG-16 neural net is fairly large and uses quite a lot of disk space and bandwidth. It should be noted, however, that most performance tests of the above mentioned CNN&rsquo;s have focused on their capacity to correctly annotate/classify the content in images (faces, animals, objects, etc.), and not their ability to assist in the detection of visual similarities, which is how the CNN&rsquo;s are used in the VRD.</span></p><p class="c0"><span class="c4">In our experiments with the VRD, we have mostly relied on ResNet50, although we encourage users to experiment and examine which CNN works best for their individual video dataset. While the current version of the VRD uses the CNNs that are included in Keras, there is also no reason why additional networks could not be applied in the future, potentially further improving performance. </span></p><p class="c0 c14"><span class="c4"></span></p><h2 class="c3" id="h.kg5zmrryfs96"><span class="c6">Similarity search using Faiss</span></h2><p class="c0"><span class="c5">When the image processing with a CNN is finished, the VRD uses the so-called </span><span class="c7 c5"><a class="c10" href="https://www.google.com/url?q=https://ai.facebook.com/tools/faiss/&amp;sa=D&amp;source=editors&amp;ust=1633504748348000&amp;usg=AOvVaw0lBcx76fbXuqcfkLkQu6fe">Faiss</a></span><span class="c4">&nbsp;(Facebook AI Similarity Search) library to calculate image similarity. Faiss was first released in 2019 and as the name suggests, it is developed by Facebook. Currently, it is considered to be one of the most efficient open-source tools for conducting large-scale similarity searches. For instance, Faiss can be run on Graphical Processing Units (GPU&rsquo;s) which provides significant advantages in terms of speed. While Faiss can be used for any sort of similarity search, we use it here to identify similarities between images.</span></p><p class="c0"><span class="c4">Faiss uses the visual features that were extracted in the previous step to index and calculate the visually most similar &ldquo;neighbors&rdquo; for each video frame. In more detail, all reference images are first added to a Faiss index. New images are then compared with all images in the index, producing an arbitrarily long (as defined by the user) list of similarity neighbours. Requesting more neighbours requires more processing time, meaning that the number of neighbours that the VRD is instructed to find is a tradeoff between time and accuracy. </span></p><p class="c0"><span class="c4">The found neighbours are each sorted by distance metric from the compared image, with lower numbers representing a more similar image. The distance metric 0.0 represents the absolute closest similarity match that the VRD can identify. What distance metric represents a &ldquo;correct&rdquo; match is hard to determine in a general sense, however, as it changes with the choice of a neural network, the quality of the source material, and the number of videos/images/frames in the index, for example.</span></p><p class="c0 c14"><span class="c4"></span></p><h2 class="c3" id="h.eyruhq6jpuiy"><span class="c6">Filtering options</span></h2><p class="c0"><span class="c4">To tailor and narrowing down the search results, the beta version of the VRD includes a number of filtering options. For example, it is possible to filter out the display of any identified similarity neighbors that come from the same video. It is also possible to filter out monochrome frames from the search results, assuming that monochrome frames (such as totally black or white frames) are of less interest to the user. </span></p><p class="c0"><span class="c5">Users are also given the option to select a threshold for which distance metrics should be shown in the final matching results. For instance, the VRD may be instructed to only show frame pairs with an assigned distance metric of less than 30 000. If this threshold is accurate, it should greatly reduce the number of shown non-matching frames. </span></p><h2 class="c3 c18" id="h.6p4c521lkrfo"><span class="c6"></span></h2><h2 class="c3" id="h.68l2nu8a3qm3"><span class="c6">Matching results</span></h2><p class="c0"><span class="c4">When the VRD&rsquo;s image processing and filtering is finished, the final matching results are shown to the user in textual and visual form. </span></p><p class="c0"><span class="c4">For instance, the user can study the matching results in tables that show which individual frames received the most identified similarity neighbours, or what frame pairs (i.e. comparisons between two different frames stemming from two different videos) received the lowest distance metrics.</span></p><p class="c0"><span class="c4">Users can also explore visual samples of identified similarity matches between videos, by studying miniature images of the analyzed frames.</span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 601.70px; height: 228.00px;"><img alt="" src="images/image3.png" style="width: 601.70px; height: 228.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c28">Example of what the VRD&rsquo;s outputs of matching results for individual frames may look like. The table on top shows a list of the two most similar frames that were identified for the reference frame called SF2061B-006.mov (which is shown on top). Below, visual previews of the same frames are displayed with the reference video is placed furthest to the left. In the top left corner of each image, the distance metric for the relevant frame </span><span class="c22 c28">vis-a-vis</span><span class="c28">&nbsp;the reference frame is presented (image courtesy: European History Reloaded Team). </span></p><p class="c0"><span class="c4">In addition, the VRD is equipped with possibilities to preview so-called sequential frame matches. We define a sequential match as an instance where two or more sequential frames (i.e. frames that were extracted one after the other from the original files) from two videos have been given a distance metric below a specified value. </span></p><p class="c0"><span class="c4">If frame 1-6 in Video X and frame 11-16 in Video Y are each given a distance metric below the threshold 20 000, for example, this may be defined as a sequential match. </span></p><p class="c0"><span class="c4">Sequential filtering can be used to identify instances when longer chunks of moving images have been reused, &nbsp;and can for example be filtered according to the length of sequences (such as 3, 4, or 5 seconds long sequential matches), or identified sequential matches for individual videos. </span></p><p class="c0"><span class="c4">On the whole, the above-mentioned lists, tables, and visual frame comparisons are meant to function as a guide that can point users towards videos that might be interesting to study in more detail &ndash; primarily by actually opening the original video files and viewing the moving images. </span></p><p class="c0"><span class="c5">In other words, we strongly advise against exporting and using these statistics as absolute proof of video reuse and instead encourage users to approach them as an </span><span class="c5 c22">assistance tool</span><span class="c4">&nbsp;in navigating large video datasets.</span></p><p class="c0 c14"><span class="c4"></span></p><h2 class="c3" id="h.m2qscopcov06"><span class="c6">Toolkit limitations</span></h2><p class="c0"><span class="c4">In its current version, the VRD only analyzes visual content and disregards sound. This means that the tool cannot detect if the audio track of a video file has been reused and reappears somewhere else. The rationale for this is that reused video is often dubbed/overlayed with different audio, meaning that the audio might not match even in cases where the video is reused. Since we were originally interested in performing visual similarity searches, adding audio features to the VRD has not been a priority at this stage. There is no reason why sound recognition technologies could not be incorporated in the tool in the future, however. </span></p><p class="c0"><span class="c5">By testing and analyzing the VRD&rsquo;s matching results against the </span><span class="c7 c5"><a class="c10" href="https://www.google.com/url?q=http://www.yugangjiang.info/research/VCDB/index.html&amp;sa=D&amp;source=editors&amp;ust=1633504748353000&amp;usg=AOvVaw1fWRKPaLh15Gev6bH6SH7_">VCDB dataset</a></span><span class="c4">&nbsp;(a large collection of remixed and manually annotated videos originating from video platforms such as YouTube), we have identified a few areas where the tool currently produces subpar results. One instance when the VRD&rsquo;s performance generally suffers is when the source video is of very low quality, such as when it is heavily distorted/modified, has a low resolution, or has heavy textual or symbolic overlays (such as subtitles, logotypes, and news show banners). </span></p><p class="c0"><span class="c4">Furthermore, the VRD has difficulties recognizing content if it is faced with the challenge of identifying content shown on a hand-filmed TV or computer screen (a problem that is generally known as picture-in-picture). Another source of concern has been video content with large amounts of text since text can easily be matched against text in other videos, even if the precise use of words is not identical. </span></p><p class="c0"><span class="c4">Regardless of these limitations, however, there is still a good chance that the VRD will correctly identify even highly modified or overlaid content &ndash; a chance that can in many cases be further improved by pre-processing.</span></p><p class="c0"><span class="c5">Finally, it is important to point out that the VRD produces an </span><span class="c5 c22">estimation </span><span class="c4">of patterns of similarity across video files. This means that its outputs/results should not be interpreted as a definitive measurement of similarity or a final and absolute proof of video reuse. </span></p><p class="c0 c14"><span class="c26"></span></p><p class="c0"><span class="c17">///</span></p><h1 class="c0 c20" id="h.a8ib17svlt9d"><span class="c8">Install</span></h1><h2 class="c0 c20" id="h.rwb267ta71be"><span class="c4">** Work in progress **</span></h2><p class="c0"><span class="c4">This section will be updated as soon as the VRD is officially released.</span></p><p class="c0 c14"><span class="c4"></span></p><p class="c0"><span class="c17">///</span></p><h1 class="c0 c20" id="h.9be9akvpktj1"><span class="c8">Contact</span></h1><p class="c0"><span class="c5">Follow our project on </span><span class="c7 c5"><a class="c10" href="https://www.google.com/url?q=https://github.com/humlab/vrd&amp;sa=D&amp;source=editors&amp;ust=1633504748356000&amp;usg=AOvVaw3qj6rnxHliIKqTqqVcpU-Q">GitHub</a></span><span class="c5">&nbsp;and the </span><span class="c7 c5"><a class="c10" href="https://www.google.com/url?q=https://www.cadeah.eu/&amp;sa=D&amp;source=editors&amp;ust=1633504748356000&amp;usg=AOvVaw2UFUbJqwkDQ2RE9jYtXnGU">European History Reloaded website</a></span><span class="c4">. </span></p><p class="c0"><span class="c5">Don&rsquo;t hesitate to reach out to </span><span class="c7 c5"><a class="c10" href="mailto:tomas.skotare@umu.se">tomas.skotare@umu.se</a></span><span class="c5">&nbsp;or </span><span class="c7 c5"><a class="c10" href="mailto:maria.c.eriksson@umu.se">maria.c.eriksson@umu.se</a></span><span class="c4">&nbsp;if you have any questions or comments.</span></p><p class="c0 c14"><span class="c4"></span></p><p class="c14 c24"><span class="c16"></span></p></body></html>